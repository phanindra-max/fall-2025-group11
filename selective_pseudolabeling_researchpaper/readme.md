- **Semi-Supervised Domain Adaptation (SSDA)** is a machine learning setup that deals with the challenge of transferring knowledge from one domain (called the source domain) to another (called the target domain) when:
  - The source domain has a large number of labeled examples.
  - The target domain has a small number of labeled examples and a large number of unlabeled examples.
  - The data distributions between the source and target domains are different, but related.
  
- The research paper covers:
  - **Data of concern**: 
    - There is a source domain (e.g., product images on white backgrounds) with lots of labeled data 
    - A target domain (e.g., real-world photos) with: 
      - A small amount of labeled data 
      - A large amount of unlabeled data 
    - This is a common setup in real-world applications (e.g., autonomous driving, medical imaging), where labeling the target domain is expensive.
  - **Problem** : Pseudo-Labeling Is Fragile
    - A common way to handle this is pseudo-labeling: Train a model, then use it to assign labels to unlabeled target samples, and retrain on those. 
    - But:
      - The Pseudo-labels can be wrong, especially in early stages.
      - Most systems just use confidence thresholds to decide whether to trust pseudo-labels.
      - This leads to a bias toward easy examples, missing harder but important ones.
      - Errors get amplified when noisy pseudo-labels are added.
  - **Approach**: They propose a Reinforcement Learning-based selection agent that learns which unlabeled examples to pseudo-label and include in training. Few key innovations are:
    - Selective Pseudo-Labeling: Instead of just taking all high-confidence examples, they use a Q-learning agent to selectively choose which pseudo-labeled examples to include in training.
    - Target Margin Loss (TML): To better use the few labeled target samples, they introduce a margin-based loss that pushes their features to be more discriminative, using angular margins (like in ArcFace).
  - **Reward Design for RL Agent**: The agent is rewarded not just for confidence, but for:
    - Classifier confidence on selected sample 
    - Its representativeness (how central it is to the class)
    - The reduction in entropy across the unlabeled set when it’s added — encouraging global improvements 
    - So the agent learns over time which kinds of examples help most when added.
- Feature -> Linear Layer -> Softmax -> Loss  : normal NN
- Instead of normal softmax(raw scores/logits to probabilities) + loss (pred probs to true labels) like cross entropy, margin based softmax + loss is used for target domain.


- **Regular softmax** : take logits, calculate probabilities using softmax and then calculate loss (simply: make the score/logits of correct class higher). eg: If Class 0 logit: z0 = 2.0, Class 1 logit: z1 = 1.5. Then simply calculate $$softmax(z0)=e2.0/(e2.0+e1.5)≈0.62, softmax(z1)=e1.5/(e2.0+e1.5)≈0.38 $$ If true class is 0, loss will be -log(0.62)≈0.47
- **Margin based softmax**: take logits, subtract them with margin for true class, calculate probabilities using softmax and then calculate loss(simply: make the score/logits of correct class higher but with a margin/create a gap). For the same example as above, Class 0 logit will be z0 = 2.0 - 0.5 = 1.5 (margin), Class 1 logit: z1 = 1.5. softmax will be calculated same, and it will be softmax(z0) = 0.5, softmax(z1) = 0.5 and loss will be -log(0.5) = 0.693. 
- So basically in margin based softmax, we are trying to reduce the logit by introducing margin, making it **harder** for model to increase that logit and reduce loss again. Over time, the model learns to push logits of the correct class far above the rest, making it more robust to ambiguity or noise.
- **Angular-based margin** : instead of reducing logit directly, angular
  - We know that z = wx + b where b= bias, w = weight matrix, x = input feature vector. so z -> softmax -> loss
  - classification layer does z = s.Cos($\theta$) where s is scaling factor, Cos($\theta$) = $\frac{w \cdot x}{\|w\| \|x\|}$. $\theta$ measure the angle between feature vector and class weight vector. usually under range [-1,1] and measures how close x and w are aligned. We want both go them to be aligned as close as possible.
  - So angle is small -> well aligned -> high confidence. angle small -> cosine is high -> high score(logit)
  - Now in angular based margin, this z is z = s.Cos($\theta + m$) now we increase the $\theta$ value for correct class by a margin, thereby reducing the z value for it.

- Annotation cost refers to the total expenses involved in the process of labeling or tagging data for use in training machine learning models
- Selective pseudo-labeling: This simple strategy can select more accurate samples but non-necessary the most useful samples for the prediction model. For example, the more confident and accurate samples may be closer to the labeled data and fail to represent the distribution of the target domain. - This means that the strategy can select more accurate samples but not necessarily the most useful samples for prediction model. eg: Just picking the most confident pseudo-labeled examples might give you accurate labels, but it doesn’t help the model learn new patterns. Those samples are probably very similar to your already-labeled ones, so they don’t help the model explore the full range of the target data. That limits generalization.
- to address it , RL based selective pseudo-labeling is proposed.
- Domain adversial Learning: used in domain adaptation where model learns to make features look similar across domains without needing labelled data from target domain.
  - Problem It's Solving: In domain adaptation, we have:
    - A source domain (e.g., U.S. street signs) with lots of labeled data
    - A target domain (e.g., European street signs) with few or no labels
    - Your goal is to train a model that performs well on the target domain, even though most of your supervision is from the source. But here’s the catch: The source and target data have different distributions. So a model trained on the source doesn't generalize well to the target.
  - Idea of Domain Adversarial Learning: We want the model to:
    - Classify examples correctly (source)
    - But also hide domain information — so it can’t be easily told apart whether an input comes from source or target
  - How It Works (3 Parts):
    - 1. Feature Extractor: Learns features from input data (shared for both source and target)
    - 2. Label Predictor: Predicts class labels (only trained using source labeled data)
    - 3. Domain Discriminator: Tries to predict which domain (source or target) a feature comes from
    - Now here’s the trick: We train the feature extractor to fool the domain discriminator!
    - Like a game: The domain discriminator gets better at telling domains apart. The feature extractor learns to generate features that confuse the discriminator (i.e., domain-invariant features)
    - This is adversarial learning — two parts competing:
      - One tries to tell domains apart
      - The other tries to make features look domain-neutral
  - Loss Functions: You optimize two objectives:
    - Classification loss — so the model predicts correct labels (on source)
    - Adversarial loss — domain discriminator loss is reversed for the feature extractor (using a gradient reversal layer or similar trick)
  - What You Get: The result is:
    - Features that are good for classification
    - But also domain-invariant — they work similarly for both source and target data
    - So when you run the classifier on target domain examples, it performs better — even without target labels.
- Paper for DANN : "Domain-Adversarial Training of Neural Networks" by Ganin et al., 2016